{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch-lightning","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pytorch-Lightning is the best library for Structuring PyTorch Code\n\n*   No Boiler Code like - `.to(device)`,`.step()`,`.backward()`\n*   Simplly implement train,validation,testing loops \n*   Change device from cpu to gpu or tpu by changing single argument.\n*   Separate modelling and data handling/processing\n*   Every epoch checkpoints can be stored and visualized in tensorboard \n*   Resume your training model any time from saved checkpoints. \n\n","metadata":{}},{"cell_type":"code","source":"import pytorch_lightning as pl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# This is basic look of pytorch-lighning\nThere are many reserved methods in the lighningmodules called **hooks**:\n- `configure_optimizers` - this should return optimizer(Adam/SGD)\n- `training_step` - training loop, takes `batch` and `batch_idx` as parameters\n- `validation_step`-validation loop, takes `batch` and `batch_idx` as parameters\n- `testing_step`- testing loop, takes `batch` and `batch_idx` as parameters\n","metadata":{}},{"cell_type":"code","source":"class LightningModel(pl.LightningModule):\n  def __init__(self):\n    super().__init__()\n    \n  def forward(self,x):\n    pass\n  \n  def configure_optimizers(self):\n    pass\n  \n  def loss_fn(self,output,target):\n    pass \n  \n  def training_step(self):\n    pass\n  \n  def validation_step(self):\n    pass\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pytorch-lightning has data module extension that structures your data preprocessing. This structure helps to read and understand code easily for everyone.\nIt helps to reuse data across multiple projects even with complex data transform and multiple-GPU handling\n\n**Hooks :**\n\n- `train_dataloader()`\n- `val_dataloader()`\n- `test_dataloader()`\n\nAbove methods in lightning datamodule are dataloaders \n\n- `prepare_data()` - Download and tokenize or do preprocessing on complete dataset, because this is called on single gpu if your using mulitple gpu, data here is not shared accross gpus. \n- `setup()` - splitting or transformations etc. setup takes stage argument `None` by default or `fit` or `test` for training and testing respectively. \n","metadata":{}},{"cell_type":"code","source":"class LightningDataset(pl.LightningDataModule):\n  def __init__(self):\n    super().__init__()\n  \n  def prepare_data(self):\n    pass\n  \n  def setup(self,stage=None):\n    pass\n  \n  def train_dataloader(self):\n    pass\n  \n  def val_dataloader(self):\n    pass\n  \n  def test_dataloader(self):\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* There are mainly two classes required to train model and test it on test data. \n\n*  Now, we can use `pl.Trainer` for instantiating trainer with many parameters like `max_epochs`, `tpu_cores`,`num_gpus`,`cpu`(default),`logger` etc\n*  `trainer.fit`, fitting the data on model\n\n","metadata":{}},{"cell_type":"code","source":"dataset = LightningDataset()\ncassava_model = LightningModel()\ntrainer = pl.Trainer(max_epochs=10)\ntrainer.fit(model=cassava_model,datamodule=dataset)\n#trainer.test(test_dataset) \"use this if you have defined test in setup() Hook\" ","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]}]}